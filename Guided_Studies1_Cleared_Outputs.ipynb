{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ST6VJ2Kmqa_"
   },
   "source": [
    "# 10-K Filings Analysis Workflow\n",
    "\n",
    "This notebook is dedicated to the analysis of 10-K filings obtained from the [SEC EDGAR database](https://www.sec.gov/edgar.shtml). To enhance efficiency, we're leveraging pre-cleaned 10-K documents provided by the [Notre Dame Software Repository for Accounting and Finance (SRAF)](https://sraf.nd.edu/sec-edgar-data/cleaned-10x-files/). These pre-processed documents reduce the initial data cleaning burden and facilitate a more focused analysis.\n",
    "\n",
    "## Step 1: Acquisition and Initial Filtering of 10-K Documents\n",
    "\n",
    "The initial phase involves downloading zip files from the SRAF, which contain a mix of 10-Q and 10-K filings for publicly traded companies in the U.S. Our goal in this step is to sift through these files to retain only the 10-K filings for subsequent analysis. The steps include:\n",
    "\n",
    "1. Decompressing all zip files to access the contained documents.\n",
    "2. Scanning each `.txt` document to identify 10-K filings, utilizing their uniform naming convention for identification.\n",
    "3. Segregating and relocating any files that are not 10-Ks to a distinct directory, ensuring our primary working directory contains solely 10-K documents for each entity and corresponding fiscal year.\n",
    "\n",
    "Following this, a selection of 10-K filings will be randomly chosen to form the sample size for detailed analysis.\n",
    "\n",
    "## Step 2: Extraction of Specific Content from 10-K Filings\n",
    "\n",
    "Our analysis is particularly focused on the \"Risk Factors\" segment, identified as Item 1A in 10-K filings. This section offers insights into the potential risks and challenges companies may face. To isolate this information, the following steps will be undertaken:\n",
    "\n",
    "1. Thorough examination of each 10-K document to locate the \"Risk Factors\" section.\n",
    "2. Extraction of the \"Risk Factors\" content from each document and saving it separately for in-depth analysis. This may involve saving the information in a new file or a database, depending on the requirements of the subsequent analysis.\n",
    "3. Optionally, the original 10-K documents, post-extraction of the relevant sections, can be moved to a different directory for archiving purposes.\n",
    "\n",
    "This focused approach on Item 1A aims to elucidate the risk landscapes of various companies, offering valuable insights into their operational and strategic vulnerabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6uNjDvAxpRL"
   },
   "source": [
    "next step as mentioned befor check if item 1a is inside the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PMykhWMoJWj",
    "outputId": "e6675f73-b5ff-4dfb-c2a0-524574d67a2e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re  # Import the regular expressions library\n",
    "\n",
    "def contains_keyword(file_path, pattern):\n",
    "    \"\"\"Check if the file contains the given pattern using regex.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "            # Use re.search to look for the pattern in the file content\n",
    "            if re.search(pattern, file_content, re.IGNORECASE):\n",
    "                return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "    return False\n",
    "\n",
    "def move_files_without_keyword(source_dir, dest_dir, pattern):\n",
    "    \"\"\"Move files that do not contain the regex pattern to the destination directory.\"\"\"\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    # Counters for reporting\n",
    "    total_files = 0\n",
    "    moved_files = 0\n",
    "\n",
    "    # Iterate over all .txt files in the source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(source_dir, filename)\n",
    "\n",
    "            # If the file does not contain the keyword pattern, move it to the destination directory\n",
    "            if not contains_keyword(file_path, pattern):\n",
    "                shutil.move(file_path, os.path.join(dest_dir, filename))\n",
    "                moved_files += 1\n",
    "\n",
    "    print(f\"Total .txt files processed: {total_files}\")\n",
    "    print(f\"Files moved to '{dest_dir}': {moved_files}\")\n",
    "\n",
    "# Get the current working directory\n",
    "project_root_dir = os.getcwd()\n",
    "\n",
    "# Define the source and destination directories relative to the current working directory\n",
    "source_dir = os.path.join(project_root_dir, 'SAMPLE_10Ks')\n",
    "dest_dir = os.path.join(source_dir, '10K without item 1A')\n",
    "\n",
    "# Define the regex pattern for \"Item 1A\" accounting for common variations\n",
    "pattern = r'Item\\s+1[Aa]'\n",
    "\n",
    "# Move files that do not contain the pattern\n",
    "move_files_without_keyword(source_dir, dest_dir, pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAG_wTGWx0Ea"
   },
   "source": [
    "renameing them\n",
    "\n",
    "To rename each .txt file in the /content/drive/MyDrive/FrankfurtSchool/Guided_Studies_in_Financial_Management/SAMPLE_10Ks directory based on the pattern year_name_cik by extracting the year from the filename, and the company name and CIK number from the file's content, you can follow these steps:\n",
    "\n",
    "Iterate through each .txt file in the specified directory.\n",
    "Extract the year from the first 4 characters of the filename.\n",
    "Read the file's content to find the company name and CIK number using regex.\n",
    "Construct the new filename using the year_name_cik pattern.\n",
    "Rename the file to the new filename.\n",
    "Here's a Python script that implements these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVnwunBiwpXU",
    "outputId": "37b7bdc7-633f-4b42-92d7-db2287d20f44"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def extract_info_from_content(file_content):\n",
    "    \"\"\"Extract the company name and CIK number from the file's content.\"\"\"\n",
    "    # Regex patterns for company name and CIK\n",
    "    company_pattern = r'COMPANY CONFORMED NAME:\\s+(.*)\\s'\n",
    "    cik_pattern = r'CENTRAL INDEX KEY:\\s+(\\d+)\\s'\n",
    "\n",
    "    # Find company name\n",
    "    company_match = re.search(company_pattern, file_content)\n",
    "    company_name = company_match.group(1) if company_match else None\n",
    "\n",
    "    # Normalize company name for filename (remove disallowed characters and shorten)\n",
    "    if company_name:\n",
    "        company_name = re.sub(r'[^\\w\\s]', '', company_name)  # Remove non-alphanumeric characters\n",
    "        company_name = re.sub(r'\\s+', '_', company_name)  # Replace spaces with underscores\n",
    "        company_name = company_name[:50]  # Limit length for simplicity\n",
    "\n",
    "    # Find CIK\n",
    "    cik_match = re.search(cik_pattern, file_content)\n",
    "    cik = cik_match.group(1) if cik_match else None\n",
    "\n",
    "    return company_name, cik\n",
    "\n",
    "def rename_files_in_directory(directory):\n",
    "    \"\"\"Rename files in the specified directory based on the year, company name, and CIK.\"\"\"\n",
    "    failed_files = []  # List to store files that failed to rename\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Extract year from the filename\n",
    "            year = filename[:4]\n",
    "\n",
    "            # Construct the full path to the file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            try:\n",
    "                # Read the file's content\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "                    # Extract company name and CIK from the content\n",
    "                    company_name, cik = extract_info_from_content(content)\n",
    "\n",
    "                    if company_name and cik:\n",
    "                        # Construct the new filename\n",
    "                        new_filename = f\"{year}_{company_name}_{cik}.txt\"\n",
    "                        new_file_path = os.path.join(directory, new_filename)\n",
    "\n",
    "                        # Rename the file\n",
    "                        os.rename(file_path, new_file_path)\n",
    "                        print(f\"Renamed '{filename}' to '{new_filename}'\")\n",
    "                    else:\n",
    "                        # Log the failure and add the file to the failed_files list\n",
    "                        failed_files.append(filename)\n",
    "                        print(f\"Failed to rename '{filename}': Missing company name or CIK\")\n",
    "            except Exception as e:\n",
    "                failed_files.append(filename)\n",
    "                print(f\"Error processing '{filename}': {e}\")\n",
    "\n",
    "    # Return the list of files that failed to rename for further investigation\n",
    "    return failed_files\n",
    "\n",
    "# Specify the directory containing the 10K files\n",
    "project_root_dir = os.getcwd()\n",
    "directory = os.path.join(project_root_dir, 'SAMPLE_10Ks')\n",
    "\n",
    "# Rename the files in the directory and get the list of files that failed to rename\n",
    "failed_files = rename_files_in_directory(directory)\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\nFiles that could not be renamed: {len(failed_files)}\")\n",
    "    for file in failed_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"\\nAll files were successfully renamed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PREF3F5SRyJR"
   },
   "source": [
    "test if all the files have been renamed succesfully\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j704TfIPRx9W",
    "outputId": "5f589bf5-bf1d-4511-d0e6-934d07a946ab"
   },
   "outputs": [],
   "source": [
    "failed_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r41EE91BSeKb"
   },
   "source": [
    "placeholder for above if this would not be the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQXsLctiTbJz"
   },
   "source": [
    "cut the item 1a estimation and collect it in a new txt and for every 5000 a new txt is created:\n",
    "\n",
    "this does not make sense and needs to be changed, becasue later we need to individually analyse the item 1a per company per year\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFYittkaRLHV",
    "outputId": "a1f962cb-4df2-4133-9ae9-2286111bf331"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the source and output directories\n",
    "project_root_dir = Path.cwd()\n",
    "source_dir = project_root_dir / 'SAMPLE_10Ks'\n",
    "output_dir = project_root_dir / 'SAMPLE_10Ks/Item_1A_Estimations'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the target strings for \"Item 1A\" and \"Item 1B\"\n",
    "targets_1a = [\"ITEM 1A. RISK FACTORS\", \"ITEM 1A RISK FACTORS\", \"ITEM 1A.\", \"1A. RISK FACTORS\", \"1A RISK FACTORS\"]\n",
    "targets_1b = [\"ITEM 1B. UNRESOLVED STAFF COMMENTS\", \"ITEM 1B UNRESOLVED STAFF COMMENTS\", \"ITEM 1B.\", \"1B. UNRESOLVED STAFF COMMENTS\", \"1B UNRESOLVED STAFF COMMENTS\", \"1B.\", \"ITEM 2\"]\n",
    "\n",
    "# DataFrame to log files that could not be processed or do not contain \"Item 1A\"\n",
    "issues_df = pd.DataFrame(columns=['Filename', 'Reason'])\n",
    "\n",
    "def find_section_end(content, targets):\n",
    "    \"\"\"Find the end of the section by locating the nearest subsequent target.\"\"\"\n",
    "    positions = [content.find(target) for target in targets if content.find(target) != -1]\n",
    "    return min(positions) if positions else len(content)\n",
    "\n",
    "def process_files():\n",
    "    file_counter = 0  # Counter for processed files\n",
    "    output_file_index = 0  # Index for output files\n",
    "\n",
    "    for file_path in source_dir.glob(\"*.txt\"):\n",
    "        try:\n",
    "            content = file_path.read_text(encoding='utf-8', errors='ignore').upper()\n",
    "            start_positions = [content.find(target) for target in targets_1a if content.find(target) != -1]\n",
    "            start_pos = min(start_positions) if start_positions else -1\n",
    "\n",
    "            if start_pos != -1:\n",
    "                end_pos = find_section_end(content[start_pos:], targets_1b)\n",
    "                section = content[start_pos:start_pos + end_pos]\n",
    "\n",
    "                # Create a new output file for every 5000 files processed\n",
    "                if file_counter % 5000 == 0:\n",
    "                    output_file_index += 1\n",
    "                with (output_dir / f\"combined_item_1a_{output_file_index}.txt\").open('a', encoding='utf-8') as output_file:\n",
    "                    output_file.write(f\"--- {file_path.name} ---\\n{section}\\n\\n\")\n",
    "\n",
    "            else:\n",
    "                issues_df.loc[len(issues_df)] = [file_path.name, \"No 'Item 1A' section found\"]\n",
    "\n",
    "            file_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            issues_df.loc[len(issues_df)] = [file_path.name, f\"Error processing file: {e}\"]\n",
    "\n",
    "    print(f\"Processed {file_counter} files. Combined 'Item 1A' sections into {output_file_index} file(s).\")\n",
    "\n",
    "process_files()\n",
    "\n",
    "# Display or save the DataFrame of issues\n",
    "if not issues_df.empty:\n",
    "    print(\"Files with issues:\")\n",
    "    print(issues_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj4eoMN4VrAN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1Pd3k7xVrHG"
   },
   "source": [
    "now the keyword analysis can begin\n",
    "\n",
    "\n",
    "the code below has not been ajusted yet the logic stays the same, firs tthe item 1a extraction need to be individualized\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "FXq6y6tgVqYU",
    "outputId": "1f11c0be-4236-4384-fc59-a4717609545f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "keywords = [\n",
    "    \"access control\", \"cybersecurity posture\", \"information\", \"legal liability\", \"data exfiltration\",\n",
    "            \"security awareness training\", \"authorization\", \"phishing\", \"APT\", \"secure sockets layer\",\n",
    "            \"threat intelligence\", \"zero trust architecture\", \"smishing\", \"whaling\", \"supply chain attack\",\n",
    "            \"cryptojacking\", \"reputation\", \"identity management\", \"trojan\", \"security architecture\", \"firewall\",\n",
    "            \"financial fraud\", \"botnet attack\", \"result\", \"patch management\", \"IoT security\", \"ransomware\",\n",
    "            \"technology\", \"privacy breach\", \"SOC\", \"secure coding\", \"security information management\", \"NIST\",\n",
    "            \"network access control\", \"operation\", \"breach\", \"security audit\", \"hack\", \"public key infrastructure\",\n",
    "            \"DDoS\", \"malvertising\", \"CSRF\", \"endpoint security\", \"CIS Controls\", \"data privacy\", \"SIM\",\n",
    "            \"social engineering\", \"cyber warfare\", \"computer\", \"disruption\", \"spear phishing\", \"application security\",\n",
    "            \"cybersecurity strategy\", \"zero-day\", \"identity theft\", \"hardware security\", \"insider threat\",\n",
    "            \"blockchain security\", \"PKI\", \"damage\", \"ransomware-as-a-service\", \"threat hunting\",\n",
    "            \"intellectual property theft\", \"financial\", \"antivirus\", \"exploit\", \"MFA\", \"CCPA\",\n",
    "            \"cybersecurity insurance\", \"threat landscape\", \"service\", \"phishing attack\", \"cybersecurity regulation\",\n",
    "            \"RCE\", \"brand damage\", \"NAC\", \"cross-site request forgery\", \"cyber resilience\", \"system\",\n",
    "            \"risk management\", \"biometric security\", \"cybersecurity audit\", \"cyber hygiene\", \"SSL\", \"trust erosion\",\n",
    "            \"cyber law\", \"data\", \"business\", \"failure\", \"network security\", \"regulatory fines\", \"FISMA\",\n",
    "            \"vulnerability\", \"security operations center\", \"IoT attack\", \"spyware\", \"cyber espionage\",\n",
    "            \"quantum cryptography\", \"PCI DSS\", \"encryption\", \"include\", \"shadow IT\", \"cloud security\", \"malware\",\n",
    "            \"penetration testing\", \"cybersecurity framework\", \"GDPR\", \"cyber threat intelligence\", \"ISO 27001\",\n",
    "            \"incident response\", \"HIPAA\", \"mobile security\", \"security by design\", \"unauthorized\", \"loss\",\n",
    "            \"customer\", \"transport layer security\", \"security\", \"espionage\", \"secure shell\", \"RaaS\", \"digital forensics\",\n",
    "            \"security policy\", \"risk assessment\", \"remote code execution\", \"compliance violation\", \"cybersecurity policy\",\n",
    "            \"vishing\", \"SSH\", \"authentication\", \"TLS\", \"VPN\", \"fileless malware\", \"intrusion\"\n",
    "]\n",
    "\n",
    "project_root_dir = Path.cwd()\n",
    "source_dir = project_root_dir / 'SAMPLE_10Ks/Item_1A_Estimations'\n",
    "output_dir = project_root_dir / 'SAMPLE_10Ks/Keyword_Analysis'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "def count_keywords(text, keywords):\n",
    "    words = text.split()\n",
    "    return Counter(word for word in words if word in keywords)\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    if \"Item 1A\" in text or \"Item 1a\" in text:\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            company = '_'.join(parts[1:-1])\n",
    "            year = parts[-1][:4]\n",
    "        else:\n",
    "            company = 'Unknown'\n",
    "            year = 'Unknown'\n",
    "\n",
    "        keyword_counts = count_keywords(text, keywords)\n",
    "        row_data = {'Company': company, 'Year': year}\n",
    "        row_data.update(keyword_counts)\n",
    "        return row_data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_files(base_directory, max_files):\n",
    "    rows = []\n",
    "    all_files = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, filename))\n",
    "\n",
    "    random.shuffle(all_files)\n",
    "    all_files = all_files[:max_files]\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_file, file) for file in all_files]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                rows.append(result)\n",
    "                print(f\"File processed: {result['Company']} {result['Year']}\")\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No relevant files found.\")\n",
    "    else:\n",
    "        print(f\"Total files processed: {len(rows)}\")\n",
    "\n",
    "    return pd.DataFrame(rows, columns=['Company', 'Year'] + keywords)\n",
    "\n",
    "# Paths and parameters as specified\n",
    "base_directory = source_dir\n",
    "max_files = 1000\n",
    "\n",
    "# Process the files and get the results DataFrame\n",
    "results_df = process_files(base_directory, max_files)\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "resulted_csv_path = output_dir / 'Keyword_Analysis_Results.csv'\n",
    "results_df.to_csv(resulted_csv_path, index=False)\n",
    "\n",
    "print(f\"Analysis results saved to {resulted_csv_path}.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHrz7OVggcwD"
   },
   "source": [
    "use the excel which i have put in the what app group to get the logic and create the normlization and the logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GN7_o1uTVqh-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming `results_df` is already loaded and in the desired format\n",
    "# Assuming there exists a binary 'Target' column in results_df for logistic regression\n",
    "# Define your 'keywords' list based on your specific keywords of interest\n",
    "\n",
    "# Calculate total keyword frequencies across all documents\n",
    "keyword_frequencies = results_df[keywords].sum()\n",
    "\n",
    "# Calculate the total of all keyword frequencies to determine weights\n",
    "total_keyword_frequency = keyword_frequencies.sum()\n",
    "\n",
    "# Determine weights for each keyword\n",
    "keyword_weights = keyword_frequencies / total_keyword_frequency\n",
    "\n",
    "# Calculate weighted counts for each document\n",
    "for keyword in keywords:\n",
    "    weighted_column = f'{keyword}_Weighted'\n",
    "    results_df[weighted_column] = results_df[keyword] * keyword_weights[keyword]\n",
    "\n",
    "# Sum the weighted counts for each document to get a total weighted count\n",
    "results_df['Total_Weighted_Count'] = results_df[[f'{kw}_Weighted' for kw in keywords]].sum(axis=1)\n",
    "\n",
    "# Normalize the total weighted counts to get the cybersecurity score\n",
    "results_df['Cybersecurity_Score'] = (results_df['Total_Weighted_Count'] - results_df['Total_Weighted_Count'].min()) / (results_df['Total_Weighted_Count'].max() - results_df['Total_Weighted_Count'].min())\n",
    "\n",
    "# Calculate Z-Scores for Cybersecurity Scores\n",
    "results_df['Cybersecurity_Score_Z'] = zscore(results_df['Cybersecurity_Score'])\n",
    "\n",
    "# Visualization of Cybersecurity Scores\n",
    "num_bins = max(10, int(len(results_df['Cybersecurity_Score'].unique()) / 10))\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(results_df['Cybersecurity_Score'], bins=num_bins, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Normalized Cybersecurity Scores', fontsize=16)\n",
    "plt.xlabel('Normalized Cybersecurity Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "sns.despine()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define the output directory for saving the analysis results\n",
    "project_root_dir = Path.cwd()  # Gets the current working directory\n",
    "output_dir = project_root_dir / 'Analysis_Results'  # Adjust this path as needed\n",
    "output_dir.mkdir(parents=True, exist_ok=True)  # Creates the directory if it doesn't exist\n",
    "\n",
    "# Save the enhanced DataFrame with cybersecurity scores to a CSV file\n",
    "enhanced_csv_path = output_dir / \"Enhanced_Keywords_with_Scores.csv\"\n",
    "results_df.to_csv(enhanced_csv_path, index=False)\n",
    "print(f\"Enhanced analysis results with cybersecurity scores saved to {enhanced_csv_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
